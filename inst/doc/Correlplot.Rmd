---
title: "Visualization of a correlation matrix with the Correlplot package"
author:
- Jan Graffelman - Universitat Politecnica de Catalunya
- Jan de Leeuw - University of California Los Angeles
date: "`r Sys.Date()`"
output:
  html_vignette:
      toc: false
      fig_caption: yes
  pdf_document:
    highlight: null
    number_sections: no    
bibliography: ["Correlplot.bib"]
vignette: >
  %\VignetteIndexEntry{Visualization of a correlation matrix with the Correlplot package}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
knitr::opts_chunk$set(fig.align = "center")
knitr::opts_chunk$set(fig.width = 6, fig.height = 6) 
```

## Introduction

This documents gives some instructions on how to create graphical representations of a correlation matrix in the statistical environment R with package `Correlplot`, using a variety of different statistical methods (@GraffelmanDeLeeuw). We use principal component analysis (PCA), multidimensional scaling (MDS), principal factor analysis (PFA), weighted alternating least squares (WALS), correlograms (CRG) and corrgrams to produce displays of correlation structure. The next section shows how to use the functions of the package in order to create the different graphical representations. The computation of goodness-of-fit statistics is also addressed. All methods are illustrated on a single data set, the wheat kernel data introduced below.

## Graphical representations of a correlation matrix

We first load some packages we will use:

```{r preinstall}
library(calibrate)
library(corrplot)
library(Correlplot)
```

Throughout this vignette, we will use the wheat kernel
data set taken from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets/seeds) in order to illustrate the different plots. 
The wheat kernel data (@Charytanowicz) consists of 210 wheat kernels, of which the variables 
*area* ($A$), *perimeter* ($P$), *compactness* ($C = 4*\pi*A/P^2$), 
*length*, *width*, *asymmetry coefficient* and *groove* (length of kernel groove) were registered. There are 70 kernels of each of three varieties *Kama*, *Rosa* and *Canadian*; here we will only use the kernels of variety *Kama*. The data is made available with: 


```{r}
data("Kernels")
X <- Kernels[Kernels$variety==1,]
X <- X[,-8]
head(X)
```
The correlation matrix of the variables is given by:

```{r}
R <- cor(X)
round(R,digits=3)
```

### 1. The corrgram

The corrgram (@Friendly) is a tabular display of the entries of a correlation matrix that uses colour and shading to represent correlations. Corrgrams can be made with the fuction `corrplot` 


```{r corrgram, fig.cap = "A corrgram of the wheat kernel data."}
corrplot(R, method="circle",type="lower")
```


This shows most correlations are positive, and correlations with *asymmetry* are weak.

### 2. The correlogram

The correlogram (@Trosset) represents correlations by the cosines between vectors.

```{r names, echo = FALSE}
vnames <- substr(colnames(R),1,4)
colnames(R) <- rownames(R) <- vnames
colnames(X) <- vnames
```


```{r correlogram, fig.cap = "The correlogram of the wheat kernel data."}
theta.cos <- correlogram(R,xlim=c(-1.1,1.1),ylim=c(-1.1,1.1),main="CRG")
```

The vector `theta.cos` contains the angles (in radians) of each variable with respect to the positive $x$-axis. The approximation provided by these angles to the correlation matrix is calculated by

```{r}
Rhat.cor <- angleToR(theta.cos)
```

The correlogram always perfectly represents the correlations of the variables with themselves, and these have a structural contribution of zero to the loss function. We calculate the root mean squared error (RMSE) of the approximation by using function `rmse`. We include the diagonal in the RMSE calculation by setting `omit.diagonal` to `FALSE`.

```{r}
rmse.crg <- rmse(R,Rhat.cor,omit.diagonal=FALSE)
rmse.crg
```


This gives and RMSE of `r formatC(rmse.crg, digits=4, format = "f")`,
which shows this representation has a large amount of error. The correlogram can be modified by
using a linear interpretation rule, rendering correlations linear in 
the angle (@Graffelman2013). This representation is obtained by:

```{r linearcorrelogram, fig.cap="Linear correlogram of the wheat kernel data."}
theta.lin <- correlogram(R,ifun="lincos",labs=colnames(R),xlim=c(-1.1,1.1),
                         ylim=c(-1.1,1.1),main="CRG")
```

The approximation to the correlation matrix by using this linear interpretation function is calculated by


```{r}
Rhat.corlin <- angleToR(theta.lin,ifun="lincos")
rmse.lin <- rmse(R,Rhat.corlin,omit.diagonal=FALSE)
rmse.lin
```

and this gives a RMSE of `r formatC(rmse.lin, digits=4, format = "f")`. In this case, the linear representation is seen to improve the approximation. 

### 3. The PCA biplot of the correlation matrix

We create a PCA biplot of the correlation matrix, doing the calculations for a PCA by hand, using the singular value decomposition of the (scaled) standardized data. Alternatively, standard R function `princomp` may be used to obtain the coordinates needed for the correlation biplot. We use function `bplot` from package 
`calibrate` to make the biplot:

```{r pcabiplot, fig.cap = "PCA biplot of the wheat kernel data."}
n <- nrow(X)
Xt <- scale(X)/sqrt(n-1)
res.svd <- svd(Xt)
Fs <- sqrt(n)*res.svd$u # standardized principal components
Gp <- res.svd$v%*%diag(res.svd$d) # biplot coordinates for variables
bplot(Fs,Gp,colch=NA,collab=colnames(X),xlab="PC1",ylab="PC2",main="PCA")
```


The joint representation of kernels and variables emphasizes this is a biplot of the (standardized) data matrix. However, this plot is a *double biplot* because scalar products between variable vectors approximate the correlation matrix. We stress this by
plotting the variable vectors only, and adding a unit circle:

```{r pcacorrelationcircle, fig.cap = "PCA biplot of the correlation matrix."}
bplot(Gp,Gp,colch=NA,rowch=NA,collab=colnames(X),xl=c(-1,1),
      yl=c(-1,1),main="PCA")
circle()
```

The PCA biplot of the correlation matrix can be obtained from a correlation-based PCA or also directly from the spectral decomposition of the correlation matrix. The rank two approximation, obtained by means of scalar products between vectors, is calculated by:

```{r}
Rhat.pca <- Gp[,1:2]%*%t(Gp[,1:2])
```

In principle, PCA also tries to approximate the ones on the diagonal of the correlation matrix. These are included in the RMSE calculation by setting `omit.diagonal` to `FALSE`.

```{r}
rmse.pca <- rmse(R,Rhat.pca,omit.diagonal=FALSE)
rmse.pca
```
This gives a RMSE of `r formatC(rmse.pca, digits=4, format = "f")`,
which is an improvement over the previous correlograms. Function `rmse` can also be used to calculate the RMSE of each variable
separately:

```{r}
rmse(R,Rhat.pca,omit.diagonal=FALSE,per.variable=TRUE)
```

This shows that *asymmetry* has the worst fit.

### 4. The MDS map of a correlation matrix

We transform correlations to distances with the $\sqrt{2(1-r)}$ transformation, and use the `cmdscale` function from the `stats` package to perform metric multidimensional scaling. We mark negative correlations with a dashed red line.

```{r mdsplot, fig.cap = "MDS map of the correlation matrix of the wheat kernel data."}
Di <- sqrt(2*(1-R))
out.mds <- cmdscale(Di,eig = TRUE)
Fp <- out.mds$points
opar <- par(bty = "l")
plot(Fp[,1],Fp[,2],asp=1,xlab="First principal axis",
     ylab="Second principal axis",main="MDS")
textxy(Fp[,1],Fp[,2],colnames(R),cex=0.75)
par(opar)

ii <- which(R < 0,arr.ind = TRUE)

for(i in 1:nrow(ii)) {
  segments(Fp[ii[i,1],1],Fp[ii[i,1],2],
           Fp[ii[i,2],1],Fp[ii[i,2],2],col="red",lty="dashed")
}
```


We calculate distances in the map, convert these back to correlations:

```{r}
Dest <- as.matrix(dist(Fp[,1:2]))
Rhat.mds <- 1-0.5*Dest*Dest
```
Again, correlations of the variables with themselves are perfectly approximated (zero distance), and we include these in the RMSE calculations:

```{r}
rmse.mds <- rmse(R,Rhat.mds,omit.diagonal=FALSE)
rmse.mds
```

### 5. The PFA biplot of a correlation matrix

Principal factor analysis can be performed by the function `pfa` of package `Correlplot`. We extract the factor loadings.

```{r pfa}
out.pfa <- Correlplot::pfa(X,verbose=FALSE)
L <- out.pfa$La
```

The biplot of the correlation matrix obtained by PFA is in fact the same as what is known as a factor loading plot in factor analysis, to which a unit circle can be added. The approximation to the correlation matrix and its RMSE are calculated as:

```{r}
Rhat.pfa <- L[,1:2]%*%t(L[,1:2])
rmse.pfa <- rmse(R,Rhat.pfa)
rmse.pfa
```

In this case, the diagonal is excluded, for PFA explicitly avoids fitting the diagonal. To make the factor loading plot, aka PFA biplot of the correlation matrix:

```{r pfabiplot, fig.cap = "PFA biplot of the correlation matrix of the wheat kernel data."}
bplot(L,L,pch=NA,xl=c(-1,1),yl=c(-1,1),
     xlab="Factor 1",ylab="Factor 2",main="PFA",rowch=NA,
     colch=NA)
circle()
```


The RMSE of the plot obtained by PFA is `r formatC(rmse.pfa, digits=4, format = "f")`, lower than the RMSE obtained by PCA. Note that
variable *area* reaches the unit circle for having a communality of 1, or, equivalently, specificity 0, i.e. PFA produces what is known as a *Heywood case* in factor analysis. The specificities are given by:


```{r}
diag(out.pfa$Psi)
```

### 6. The WALS biplot of a correlation matrix

The correlation matrix can also be factored using weighted alternating least squares, avoiding the fit of the ones on the diagonal of the correlation matrix by assigning them weight 0, using function `ipSymLS` (@DeLeeuw). 


```{r}
W <- matrix(1,nrow(R),nrow(R))
diag(W) <- 0
Fp.als <- ipSymLS(R,w=W,eps=1e-15)
```

```{r ipsymlsplot, fig.cap = "WALS biplot of the correlation matrix of the wheat kernel data."}
bplot(Fp.als,Fp.als,rowch=NA,colch=NA,collab=colnames(R),
      xl=c(-1.1,1.1),yl=c(-1.1,1.1),main="WALS")
circle()
```

Weighted alternating least squares has, in contrast to PFA, no restriction on the vector length. If the vector lengths 
in the biplot are calculated, then variable *area* is seen to 
just move out of the unit circle.

```{r}
Rhat.wals <- Fp.als%*%t(Fp.als)
sqrt(diag(Rhat.wals))
rmse.als <- rmse(R,Rhat.wals)
rmse.als
```

The RMSE of the approximation obtained by WALS is `r formatC(rmse.als, digits=6, format = "f")`, slightly below the RMSE of PFA. The WALS low rank approximation to the correlation matrix can also be obtained by the more generic function `wAddPCA`, which allows for non-symmetric matrices and adjustments (see the next section). In order to get uniquely defined biplot vectors for each variable, corresponding to symmetric input, an eigendecomposition is applied to the fitted correlation matrix. 

```{r}
p <- nrow(R)
W <- matrix(1,p,p)
diag(W) <- 0
out.wals <- wAddPCA(R, W, add = "nul", verboseout = FALSE, epsout = 1e-10)
Rhat.wals <- out.wals$a%*%t(out.wals$b)
out.eig <- eigen(Rhat.wals)
Fp.adj <- out.eig$vectors[,1:2]%*%diag(sqrt(out.eig$values[1:2]))
rmse.als <- rmse(R,Rhat.wals)
rmse.als
```

### 7. The WALS biplot using an adjustment of the correlation matrix

A scalar adjustment can be employed to improve the approximation of the correlation matrix, and the adjusted correlation matrix is
factored for a biplot representation. That means we seek the factorization

\[
{\mathbf R}_a = {\mathbf R} - \delta {\mathbf J} = {\mathbf G} {\mathbf G}',
\]

where both $\delta$ and $\mathbf G$ are chosen such that the (weighted) residual sum of squares is minimal. This problem is solved by using function `wAddPCA`.

```{r}
p <- nrow(R)
W <- matrix(1,p,p)
diag(W) <- 0
out.wals <- wAddPCA(R, W, add = "one", verboseout = FALSE, epsout = 1e-10)
delta <- out.wals$delta[1,1]
Rhat <- out.wals$a%*%t(out.wals$b)
out.eig <- eigen(Rhat)
Fp.adj <- out.eig$vectors[,1:2]%*%diag(sqrt(out.eig$values[1:2]))
```

The optimal adjustment $\delta$ is `r formatC(delta, digits=3, format = "f")`. The corresponding biplot is shown below. 

```{r walsbiplot, fig.cap = "WALS biplot of the correlation matrix of the wheat kernel data, with the use of an additive adjustment."}
bplot(Fp.adj,Fp.adj,rowch=NA,colch=NA,collab=colnames(R),
      xl=c(-1.2,1.2),yl=c(-1.2,1.2),main="WALS adjusted")
circle()
```

Note that, when calculating the fitted correlation matrix, adjustment $\delta$ is added back. The fitted correlation matrix and its RMSE are now calculated as:

```{r}
Rhat.adj <- Fp.adj%*%t(Fp.adj)+delta
rmse.adj <- rmse(R,Rhat.adj)
rmse.adj
```

This gives RMSE `r formatC(rmse.adj, digits=4, format = "f")`. This is smaller than the RMSE obtained by WALS without adjustment. We summarize the values of the RMSE of all methods in a table below:

```{r}
rmsevector <- c(rmse.crg,rmse.lin,rmse.pca,rmse.mds,rmse.pfa,rmse.als,rmse.adj)
methods <- c("CRG (cos)","CRG (lin)","PCA","MDS",
"PFA","WALS R","WALS Radj")
results <- data.frame(methods,rmsevector)
results <- results[order(rmsevector),]
results
```

A summary of RMSE calculations for four methods (PCA and WALS, both with and without $\delta$ adjustment) can be obtained by the functions
`FitRwithPCAandWALS` and `rmsePCAandWALS`; the first applies the four methods to the correlation matrix, and the latter calculates the RMSE statistics for the four approximations. The bottom line of the table produced by `rmsePCAandWALS` gives the overall RMSE for each methods.

```{r}
output <- FitRwithPCAandWALS(R,eps=1e-15)
rmsePCAandWALS(R,output)
```



## References
